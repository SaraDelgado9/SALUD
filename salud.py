# -*- coding: utf-8 -*-
"""Salud.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13GyYaR4ms3DnD5jFimi209NXHXzIR__d

# **Cargue de librerias y datos**
"""

# Librerias generales
import pandas as pd
import numpy as np
from datetime import datetime
import seaborn as sns
import plotly.express as px
# Librerias de visualización
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix

from google.colab import drive
drive.mount('/content/drive')

df_cronicos= pd.read_csv("/content/drive/MyDrive/Analitica3/SALUD/RETO_df_cronicos.csv")
df_egresos= pd.read_csv("/content/drive/MyDrive/Analitica3/SALUD/RETO_df_egresos.csv")
df_usuarios= pd.read_csv("/content/drive/MyDrive/Analitica3/SALUD/RETO_df_usuarios.csv")

#df_cronicos= pd.read_csv("/content/drive/MyDrive/Analitica /Analítica 3/Salud/Proyecto/RETO_df_cronicos.csv")
#df_egresos= pd.read_csv("/content/drive/MyDrive/Analitica /Analítica 3/Salud/Proyecto/RETO_df_egresos.csv")
#df_usuarios= pd.read_csv("/content/drive/MyDrive/Analitica /Analítica 3/Salud/Proyecto/RETO_df_usuarios.csv")

"""# **Limpieza y tratamiento de los datos**"""

#convertir todas las columnas de un DataFrame en minisculas
def convertirmin(df):
    df.columns = map(str.lower,df.columns)

convertirmin(df_egresos)
convertirmin(df_usuarios)
convertirmin(df_cronicos)

df_cronicos

df_egresos

df_usuarios

df_cronicos.columns

df_egresos.columns

df_usuarios.columns

print('La dimensión de df_cronicos es:', df_cronicos.shape)
print('La dimensión de df_egresos es:', df_egresos.shape)
print('La dimensión de df_usuarios es:', df_usuarios.shape)

'''se decide trabajar con las primeras 122 columnas de la base de datos df_cronicos, debido a que las siguientes columnas
contienen alrededor de 60-90% de datos nulos, además se decide tener en cuenta el diágnostico principal y no los secundarios '''
df_cronicos = df_cronicos.iloc[:, :122]

df_cronicos.iloc[:,122:].isnull().sum()

df_cronicos.iloc[:,:122].isnull().sum()

df_cronicos

#se divide el df_cronicos para visualizar bien los datos nulos en cada columna
primeras_columnas = df_cronicos.iloc[:, :60]
primeras_columnas

primeras_columnas.isnull().sum()

ultimas_columnas = df_cronicos.iloc[:, 60:]
(ultimas_columnas)

ultimas_columnas.info()

#se decide hacer eliminación de variables que presenten más del 70% de nulos y otras que a simple juicio no influyen en el estudio

columnas_eliminar= ['mes','year','test findrisc', 'índice tobillo/brazo','observaciones','presión arterial registro medico',
                    'tiene riesgo de tener diabetes mellitus','espirometria','vef1/cvf', 'vef1/vfc posbroncodilatador',
                    'gravedad', 'diagnóstico epoc', 'disnea mmrc', 'clasificación', 'cat',
                    'número de exacerbaciones último año (que hayan necesitado hospitalizado)',
                    'clasificación gold', 'clasificación1', 'clasificación bodex', 'oxígeno dependiente', 'tiene gases arteriales',
                    'resultado', '¿cuál?', 'indice paquete/año', 'otras morbilidades', 'clasificación cambio de tfg', 'última mamografía',
                    'última citología', 'última cita odontológica', 'última cita con optometría / oftallmología', 'último psa',
                    'basciloscopia', 'fecha basciloscopia', 'última vacuna de influenza', 'fecha vacuna neumococo',
                    'fecha vacuna neumococo veintitrés valentes', 'es insulinorequiriente','fecha glicemia','fecha hemoglobina glicosilada',
                    'fecha ldl','fecha hdl','fecha colesterol total','fecha creatinina1','fecha microalbuminuria',
                    'fecha creatinina2','análisis y conducta a seguir','fecha próximo control','fecha cita morbilidad' ,'fecha trigliceridos']

df_cronicos = df_cronicos.drop(columnas_eliminar, axis=1)

##la saturacion de oxigeno se imputa con el promedio de las saturaciones

promedio_sat = round(df_cronicos['saturación de oxígeno (%)'].mean(),0)
df_cronicos['saturación de oxígeno (%)'].fillna(promedio_sat, inplace=True)

#Los nulos se toman como registros ND (No data)

NA_ND=['tiene próximo control', 'requiere cita de morbilidad', 'diabetes mellitus', 'tiene epoc', 'valvulopatía', 'tabaquismo','remisión',
       'úlcera de pie diabético', 'estadio de la enfermedad renal','clasificación de framinghan', 'diagnóstico principal',
       'sufre de alguna enfermedad cardiovascular', 'arritmia o paciente con dispositivo','tiene hta', 'tipo diabetes mellitus']
df_cronicos[NA_ND] = df_cronicos[NA_ND].fillna('ND')

#Los nulos se toman como registros Sin dato
NA_Sindato= ['tipo control', 'ambito según el médico']
df_cronicos[NA_Sindato] = df_cronicos[NA_Sindato].fillna('Sin dato')

#Los nulos se toman como registros No Aplica

NA_NoAplica= ['control diabetes', 'control hta', 'tiene riesgo de tener hta', 'epoc (clasificación bodex)',
             'enfermedad coronaria (en el último año)','insuficiencia cardíaca']
df_cronicos[NA_NoAplica] = df_cronicos[NA_NoAplica].fillna('ND')

#Los NA y ND se pasan al valor de 0

NA_ND_0=['glicemia', 'hemoglobina glicada', 'años de consumo', 'cuantos cigarrillos día', 'lipoproteina','hdl','colesterol total', 'trigliceridos', 'creatinina 1 consulta',
         'tasa de filtración glomerular tfg', 'cambio de tfg', 'meses de diferencia entre tfg', 'microalbuminuria',
         'creatinina 2 consulta', 'tasa de filtración glomerular tfg2','tiempo con el diagnóstico','tiempo con el diagnóstico1']

df_cronicos[NA_ND_0]= df_cronicos[NA_ND_0].fillna(0)
df_cronicos[NA_ND_0] = df_cronicos[NA_ND_0].replace('ND', 0)

##Se eliminan los registros que no cuentan con clase funcional, ya que esta es importante para el estudio
df_cronicos = df_cronicos.dropna(subset=['clase funcional'])

## se determina trabajar con los pacientes que tengan como principal dignostico la hipertension

df_cronicos['diagnóstico principal'].value_counts()

hiper = df_cronicos['diagnóstico principal'] == 'I10X - HIPERTENSION ESENCIAL (PRIMARIA)'
df_cronicos = df_cronicos[hiper]

df_cronicos

print('Se tienen', len(df_cronicos['nrodoc'].unique()), 'pacientes únicos en la base cronicos.')
print('Se tienen', len(df_usuarios['nrodoc'].unique()), 'pacientes únicos en la base usuarios.')
print('Se tienen', len(df_egresos['nrodoc'].unique()), 'pacientes únicos en la base egresos.')

'''de la base df_usuarios, tendremos en cuenta el ultimo registro que se tiene del usuario,
ya que esta base nos da info basica del usuario y no es necesario tenerla duplicada,
además se utiliza así esta como base transaccional'''

df_usuarios.shape

df_usuarios.info()

##los nulos de la edad, se imputarán con la media de las edades

promedio_edad = round(df_usuarios['edad'].mean(),0)
df_usuarios['edad'].fillna(promedio_edad, inplace=True)

#De la variable sexo pasar NA y valores nulos a ND

valores_sexo = ['F', 'M']
df_usuarios.loc[~df_usuarios['sexo'].isin(valores_sexo), 'sexo'] = 'ND'

#los registros de barrios nulos se toman como ND
df_usuarios= df_usuarios.fillna('ND')

# se crea una columna con numeros respecto al mes
meses_a_numeros = {
    'ENERO': 1,
    'FEBRERO': 2,
    'MARZO': 3,
    'ABRIL': 4,
    'MAYO': 5,
    'JUNIO': 6,
    'JULIO': 7,
    'AGOSTO': 8,
    'SEPTIEMBRE': 9,
    'OCTUBRE': 10,
    'NOVIEMBRE': 11,
    'DICIEMBRE': 12
}

df_usuarios['meses_numeros'] = df_usuarios['mes'].map(meses_a_numeros)

##tomar el ultimo registro de cada usuario
df_usuarios_ult = df_usuarios.sort_values(by=['year', 'meses_numeros'], ascending=[False, False])
df_usuarios_ult = df_usuarios.drop_duplicates(subset='nrodoc', keep='last')

df_usuarios_ult['ciclo_vital'].unique()

##para el estudio se decide trabajar con las personas que están en ciclo vital = vejez
df_usuarios_vejez=df_usuarios_ult.query('ciclo_vital=="Vejez"')

df_usuarios_vejez.shape

df_usuarios_vejez

df_usuarios_vejez.isnull().sum()

'''De df_usuarios solo tomaremos las variables nrodoc, edad, sexo,bariio,  fecha inicio al pgp. La variable clase funcional es importante
pero en df_cronicos se tiene el historico de la clasificación que se le ha dado en cada estudio'''

columnas_usuarios = ["nrodoc", "edad", "sexo", "barrio","fecha inicio al pgp"]
df_usuarios_vejez1 = df_usuarios_vejez[columnas_usuarios]

#unir df_usuarios_vejez con df_cronicos
df_temporal=pd.merge(df_usuarios_vejez1, df_cronicos, on="nrodoc",how="inner")
df_temporal

#revision df_egresos

df_egresos.shape

df_egresos.info()

eliminar_egresos=['mes', 'year','fecha posible alta','fecha recaudo','fecha camillero', 'fecha enfermeria','fecha facturacion audifarma',
                  'fecha farmacia','fecha aseo', 'pertinencia diagnostica','causa basica capitulo cod', 'nro atencion', 'nro ingreso',
                  'servicio habilitado cod','fecha ingreso servicio', 'fecha alta medica', 'dxprincipal egreso cod','dx relacionado1 cod',
                  'dx relacionado3 cod', 'dx principal egreso categoria cod','dx principal egreso capitulo cod', 'causa basica muerte cod',
                  'causa basica categoria cod', 'profesional especialidad grd']

df_egresos1 = df_egresos.drop(eliminar_egresos, axis=1)

## solo se trabaja con los registros que tienen en el plan prospectivo
df_egresos1= df_egresos1[df_egresos1['eps validada'] == 'NUEVA EPS S.A. PGP']

##se eliminan los regisrtos que no tienen fehca de ingreso a la clinica
df_egresos1= df_egresos1.dropna(subset=['fecha ingreso clinica'])

##convetir a formato fecha-hora
df_egresos1['fecha salida'] = pd.to_datetime(df_egresos1['fecha salida'])
df_egresos1['fecha ingreso clinica'] = pd.to_datetime(df_egresos1['fecha ingreso clinica'])

#calcular días estancia
df_egresos1['estancia hospitalaria'] = (df_egresos1['fecha salida']-df_egresos1['fecha ingreso clinica']).dt.total_seconds()

df_egresos1['estancia hospitalaria'] = df_egresos1['estancia hospitalaria'] / (60 * 60 * 24)

#unir df_temporal con df_egresos
df_completa=pd.merge(df_temporal, df_egresos1, on="nrodoc",how="inner")
display(df_completa)

df_completa.fillna('ND', inplace=True)

column_names = df_completa.columns.tolist()

# Imprimir la lista de nombres de columnas
print(column_names)

"""# **ANALISIS EXPLORATORIO**"""

df_completa.describe()

# Agrupa los datos por 'regimen afiliacion' y 'clase funcional' y calcula el conteo
data = df_completa.groupby(['regimen afiliacion', 'clase funcional']).size().unstack()

# Crea un gráfico de barras apiladas
ax = data.plot(kind='bar', stacked=True, figsize=(10, 6))

# Personaliza las etiquetas y el título
ax.set_xlabel('Régimen de Afiliación')
ax.set_ylabel('Número de Pacientes')
ax.set_title('Distribución de Clase Funcional por Régimen de Afiliación')

# Muestra el gráfico
plt.show()

data = df_completa.groupby(['clase funcional', 'modalidad contrato']).size().unstack()

# Normaliza los datos para obtener la proporción en lugar del conteo
data = data.div(data.sum(axis=1), axis=0)

# Crea un gráfico de barras apiladas
ax = data.plot(kind='bar', stacked=True, figsize=(10, 6))

# Personaliza las etiquetas y el título
ax.set_xlabel('Clase Funcional')
ax.set_ylabel('Proporción de Pacientes')
ax.set_title('Modalidad de Contrato por Clase Funcional')

# Muestra el gráfico
plt.show()

#relación entre los servicios admitidos y la clase funcional.
data = df_completa.groupby(['clase funcional', 'servicio admite']).size().unstack()

# Crea un gráfico de barras apiladas
ax = data.plot(kind='bar', stacked=True, figsize=(10, 6))

# Personaliza las etiquetas y el título
ax.set_xlabel('Clase Funcional')
ax.set_ylabel('Número de Pacientes')
ax.set_title('Servicios Admitidos por Clase Funcional')

# Muestra el gráfico
plt.legend(title='Servicio Admite', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

from matplotlib.ticker import scale_range
# Crea el gráfico de caja (box plot)
fig5 = px.box(df_completa, x='clase funcional', y='estancia hospitalaria', title='Relación entre Clase Funcional y Estancia Hospitalaria')

# Personaliza las etiquetas y el título
fig5.update_xaxes(title='Clase Funcional')
fig5.update_yaxes(title='Estancia Hospitalaria (días)', range=[0, 50])

# Muestra el gráfico
fig5.show()

# Comparar la clase funcional con el control de morbilidad.
# Agrupa los datos por 'clase funcional' y 'requiere cita de morbilidad' y calcula el conteo
data = df_completa.groupby(['clase funcional', 'requiere cita de morbilidad']).size().unstack()

# Crea un gráfico de barras apiladas
ax = data.plot(kind='bar', stacked=True, figsize=(10, 6))

# Personaliza las etiquetas y el título
ax.set_xlabel('Clase Funcional')
ax.set_ylabel('Número de Pacientes')
ax.set_title('Control de Morbilidad por Clase Funcional')

# Muestra el gráfico
plt.show()

categorical_distribution = df_completa['diabetes mellitus'].value_counts().reset_index()
categorical_distribution.columns = ['Categoría', 'Cantidad']

# Gráfico de barras
plt.bar(categorical_distribution['Categoría'], categorical_distribution['Cantidad'])
plt.xlabel('Categoría')
plt.ylabel('Cantidad')
plt.title('Distribución de Diabetes Mellitus')

segmented_summary = df_completa.groupby('diabetes mellitus')['estancia hospitalaria'].mean().reset_index()

# Gráfico de barras apiladas
segmented_summary.plot(kind='bar', x='diabetes mellitus', y='estancia hospitalaria')
plt.title('Promedio de Estancia Hospitalaria por Condición de Diabetes Mellitus')

plt.figure(figsize=(10, 6))
sns.boxplot(x='control diabetes', y='glicemia', data=df_completa, palette='Set3')
plt.title('Distribución de Glicemia por Control de Diabetes')
plt.xlabel('Control de Diabetes')
plt.ylabel('Glicemia')
plt.show()

#Gráfico de Barras Apiladas para Control de Hipertensión y Riesgo de Hipertensión:
cross_tab = pd.crosstab(df_completa['control hta'], df_completa['tiene riesgo de tener hta'])
cross_tab.plot(kind='bar', stacked=True, colormap='viridis', figsize=(10, 6))
plt.title('Control de Hipertensión vs. Riesgo de Hipertensión')
plt.xlabel('Control de Hipertensión')
plt.ylabel('Número de Pacientes')
plt.show()

#Histograma de Estancia Hospitalaria:
plt.figure(figsize=(10, 6))
sns.histplot(data=df_completa, x='estancia hospitalaria', bins=20, kde=True)
plt.title('Distribución de Estancia Hospitalaria')
plt.xlabel('Días de Estancia')
plt.ylabel('Frecuencia')
plt.show()

# Calcula el promedio de IMC por condición de diabetes mellitus
segmented_summary = df_completa.groupby('diabetes mellitus')['imc'].mean().reset_index()

# Personaliza el estilo del gráfico
sns.set(style="whitegrid")
plt.figure(figsize=(8, 6))

# gráfico de barras
barplot = sns.barplot(x='diabetes mellitus', y='imc', data=segmented_summary, palette="viridis")

# Etiqueta los ejes y el título
plt.xlabel("Condición de Diabetes Mellitus")
plt.ylabel("Promedio de IMC")
plt.title("Promedio de IMC por Condición de Diabetes Mellitus")

# Muestra los valores en las barras
for p in barplot.patches:
    barplot.annotate(format(p.get_height(), ".2f"), (p.get_x() + p.get_width() / 2., p.get_height()), ha = "center", va = "center", xytext = (0, 9), textcoords = "offset points")

# Muestra el gráfico
plt.show()

estancia_summary = df_completa.groupby('dx principal egreso categoria')['estancia hospitalaria'].agg(['mean', 'std', 'count']).sort_values(by='mean', ascending= False)
estancia_summary

# Filtra las filas con 'ND' en la columna 'barrio'
df_filtrado = df_completa[df_completa['barrio'] != 'ND']

# Agrupar por 'barrio' y 'dx principal egreso categoria', y calcula el conteo
barrio = df_filtrado.groupby(['barrio', 'dx principal egreso capitulo'])['dx principal egreso capitulo'].count()

tabl_barrio = barrio.rename('conteo').reset_index()

# Ordena de mayor a menor por el conteo
tabl_barrio = tabl_barrio.sort_values(by='conteo', ascending=False)

tabl_barrio

#gráfico de barras para mostrar los 10 primeros resultados
top_10_barrio = tabl_barrio.head(10)
plt.figure(figsize=(10, 6))
plt.barh(top_10_barrio['barrio'] + ' - ' + top_10_barrio['dx principal egreso capitulo'], top_10_barrio['conteo'])
plt.title('Top 10 de Categorías de Diagnóstico Principal en Barrios (Excluyendo ND)')
plt.xlabel('Conteo')
plt.ylabel('Barrio - Categoría de Diagnóstico Principal')

plt.show()

# Promedio de días de estancia hospitalaria según el diagnostico principal de egreso
promdx_estancia = df_completa.groupby('dx principal egreso categoria')['estancia hospitalaria'].mean()

tabl_promdx_estancia = promdx_estancia.sort_values(ascending=False).reset_index()
tabl_promdx_estancia

#gráfico de barras para mostrar los 10 primeros resultados
tabl_promdx_estancia['etiqueta'] = tabl_promdx_estancia['estancia hospitalaria'].astype(str) + ' - ' + tabl_promdx_estancia['dx principal egreso categoria']

#ráfico de barras para mostrar los 10 primeros resultados
top_10_estancia = tabl_promdx_estancia.head(10)
plt.figure(figsize=(10, 6))
plt.barh(top_10_estancia['etiqueta'], top_10_estancia['estancia hospitalaria'])
plt.title('Top 10 de Diagnóstico Principal de Egresos por Promedio de Estancia')
plt.xlabel('Estancia Hospitalaria')
plt.ylabel('Etiqueta')

# Muestra el gráfico
plt.show()

#promedio de días de estancia hospitalaria según la categoria de causa básica
promedio_estancia_por_causa = df_completa.groupby('causa basica categoria')['estancia hospitalaria'].mean()

estancia_causa = promedio_estancia_por_causa.sort_values(ascending=False).reset_index()
estancia_causa

import plotly.express as px

# Selecciona las 10 primeras filas (las de mayor promedio de estancia)
top_10_estancia_causa = estancia_causa.head(10)

#gráfico de barras
fig = px.bar(
    top_10_estancia_causa,
    x='estancia hospitalaria',
    y='causa basica categoria',
    orientation='h',  # Horizontal
    title='Top 10 de Promedio de Estancia Hospitalaria por Categoría de Causa Básica',
)

fig.update_xaxes(title='Promedio de Estancia Hospitalaria')
fig.update_yaxes(title='Categoría de Causa Básica')
fig.update_layout(xaxis=dict(type='category'))  # Ordenar las categorías por el valor en el eje x

# Muestra el gráfico interactivo
fig.show()

"""# **Escalamiento de variables**"""

from sklearn.preprocessing import LabelEncoder, OneHotEncoder
le = LabelEncoder()

variables_y_tipos = [(column, dtype) for column, dtype in zip(df_completa.columns, df_completa.dtypes)]

# Muestra la lista completa de variables y sus tipos
for variable, tipo in variables_y_tipos:
    print(f"Variable: {variable}, Tipo: {tipo}")

df_completa['demora asignacion cama'].value_counts()

# Elimina todas las filas que contienen "ND" en la columna "variable_a_limpiar"
df_completa = df_completa[df_completa['índice de fragilidad groningen'] != 'ND']

# Tranformar variables categoricas
categorical_features =['sexo','barrio','tipo','clasificación imc','diabetes mellitus','tipo diabetes mellitus','control diabetes','tiene hta','control hta','tiene riesgo de tener hta',
                       'tiene epoc','epoc (clasificación bodex)','enfermedad coronaria (en el último año)','insuficiencia cardíaca','valvulopatía','arritmia o paciente con dispositivo',
                       'sufre de alguna enfermedad cardiovascular','tabaquismo', 'clasificación de framinghan','estadio de la enfermedad renal','clase funcional','remisión','diagnóstico principal',
                       'tiene próximo control','tipo control','requiere cita de morbilidad','ambito según el médico','servicio habilitado','tipo identificacion','regimen afiliacion','eps validada',
                       'modalidad contrato','fuente financiacion1','fuente financiacion2','bloque anterior','piso anterior','servicio anterior','nro cama anterior','via ingreso','bloque','unidad estrategica',
                       'piso','nro cama','servicio egreso','tipo egreso','demora asignacion cama','demora aplicacion medicamento','demora salida clinica (dias)','transfusion sangre','antibiotico',
                       'antibiotico','alta medica','posible alta','dxprincipal egreso','dx relacionado1','dx relacionado2 cod','dx relacionado2','dx relacionado3','tipo diagnostico principal','dx principal egreso categoria','dx principal egreso capitulo','causa basica muerte',
                       'causa basica capitulo','profesional especialidad','calificación (apoyo monopodal)','calificación (índice de fragilidad)','calificación velocidad','servicio admite','causa basica categoria','úlcera de pie diabético']

for i in categorical_features:
    df_completa[i] = le.fit_transform(df_completa[i])

df_completa = df_completa.drop(columns= ['nrodoc','fecha salida', 'fecha ingreso clinica', 'bloque anterior','fecha nacimiento','bloque','nro cama anterior','nro cama','piso','piso anterior',
                                         'fecha diligenciamiento','fecha inicio al pgp','barrio','perímetro muslo','perímetro cintura','pliegue triceps','pliegue abdomen','pliegue muslo','hora','cuantos cigarrillos día'])

# Cálculo de correlación entre variables numericas
df_completa[['estancia hospitalaria','edad','peso','talla','imc','saturación de oxígeno (%)','sumatoria pliegues','presión arterial sistólica','presión arterial diastólica','frecuencia cardíaca en reposo',
             'auto-calificacion nivel de ejercicio','constantes','mets -índice metabólico','vo2 - máxima cantidad de oxígeno','tiempo en segundos (apoyo monopodal)','tiempo en segundos (recorrer 5 metros)',
             'velocidad (m/s)','glicemia','hemoglobina glicada','años de consumo','lipoproteina','hdl','colesterol total','trigliceridos','creatinina 1 consulta','tasa de filtración glomerular tfg',
             'microalbuminuria','hormona estimulante de la tiroides (tsh)','creatinina 2 consulta','tasa de filtración glomerular tfg2','meses de diferencia entre tfg','cambio de tfg','índice de fragilidad groningen','tiempo con el diagnóstico','tiempo con el diagnóstico1']].corr()

#df_completa=pd.get_dummies(df_completa)

# División de datos en var dependiente e independientes
X = df_completa.drop(columns= ['estancia hospitalaria'])
Y = df_completa['estancia hospitalaria']

X

from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import Lasso
from sklearn.preprocessing import StandardScaler
# Estandarizar los datos
standard_scaler = StandardScaler()
X_std = standard_scaler.fit_transform(X)

# Selector de variables con Lasso
sel_ = SelectFromModel(Lasso(alpha=0.05), max_features=50)
sel_.fit(X_std, Y)
print(sel_.estimator_.coef_)

#Obtener variables seleccionadas
X_new = sel_.get_support()

df_modelos = X.iloc[:,X_new]
df_modelos.head()

df_modelos.columns

X_std

"""# **División de los datos de entrenamiento y prueba**"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_std, Y, test_size=0.20, random_state=42)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

y_test

"""# **Modelos**

## **Random Forest**
"""

##librerías

from sklearn.tree import DecisionTreeRegressor
from sklearn import metrics
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.ensemble import RandomForestRegressor
import xgboost
from xgboost import XGBRFRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_absolute_percentage_error

# Training
rf = RandomForestRegressor(n_estimators = 100)
rf.fit(X_train, y_train)

# Prediction
rf_prediction =  rf.predict(X_test)

mape=mean_absolute_percentage_error(y_test,rf_prediction)
mae=mean_absolute_error(y_test,rf_prediction)
print(mae)
print(mape)

# Obtener los hiperparámetros actuales del modelo
rf_params = rf.get_params()

# Imprimir los hiperparámetros
for key, value in rf_params.items():
    print(f'{key}: {value}')

#Mejorar el modelo de random forest
from sklearn.model_selection import GridSearchCV
# aplicar grilla a random forest
rf = RandomForestRegressor()
param_grid = {'n_estimators': [50, 100, 200, 300, 400, 500],
              'max_depth': [3, 5, 7, 9, 11, 13, 15]}
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)
grid_search.fit(X_train, y_train)
# mostrar mejores parámetros
grid_search.best_params_

# Entrenar el modelo con los mejores hiperparámetros
best_rf_model = RandomForestRegressor(max_depth= 15,n_estimators=500)
best_rf_model.fit(X_train, y_train)

# Realizar predicciones con el modelo optimizado
best_rf_prediction = best_rf_model.predict(X_test)

mape=mean_absolute_percentage_error(y_test,best_rf_prediction)
mae=mean_absolute_error(y_test,best_rf_prediction)
print(mape)
print(mae)

"""## **XGBREGRESSOR**"""

#Xgboost regressor
import xgboost
xgb =  xgboost.XGBRFRegressor()

# Training
xgb.fit(X_train, y_train)

# Prediction
xgb_prediction = xgb.predict(X_test)

mape=mean_absolute_percentage_error(y_test,xgb_prediction)
mae=mean_absolute_error(y_test,xgb_prediction)
print(mape)
print(mae)

xgb_params = xgb.get_params()

# Imprimir los hiperparámetros
for key, value in xgb_params.items():
    print(f'{key}: {value}')

import xgboost as xgb
from sklearn.model_selection import GridSearchCV

# Crear un modelo XGBoost Regressor
xgb_model = xgb.XGBRFRegressor()

# Definir una cuadrícula de hiperparámetros que deseas optimizar
param_grid = {
    'n_estimators': [100, 200, 300],  # Número de árboles en el bosque
    'max_depth': [3, 4, 5],          # Profundidad máxima de los árboles
    'learning_rate': [0.01, 0.1, 0.2],  # Tasa de aprendizaje
    'subsample': [0.8, 0.9, 1.0],      # Proporción de muestras utilizadas para entrenar cada árbol
}

# Crear un objeto GridSearchCV
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')

# Realizar la búsqueda en cuadrícula en los datos de entrenamiento
grid_search.fit(X_train, y_train)

# Obtener los mejores hiperparámetros encontrados por la búsqueda en cuadrícula
best_params = grid_search.best_params_
print("Mejores hiperparámetros encontrados:", best_params)

# Entrenar el modelo con los mejores hiperparámetros
best_xgb_model = xgb.XGBRFRegressor(**best_params)
best_xgb_model.fit(X_train, y_train)

# Realizar predicciones con el modelo optimizado
xgb_prediction = best_xgb_model.predict(X_test)

mape=mean_absolute_percentage_error(y_test,xgb_prediction)
mae=mean_absolute_error(y_test,xgb_prediction)
print(mape)
print(mae)

"""## **DecisionTreeRegressor**"""

dt = DecisionTreeRegressor()

# Training
dt.fit(X_train, y_train)

# Prediction
dt_prediction = dt.predict(X_test)

mape=mean_absolute_percentage_error(y_test,dt_prediction)
mae=mean_absolute_error(y_test,dt_prediction)
print(mape)
print(mae)

#Mejora del modelo

dt = DecisionTreeRegressor()

# Definir una cuadrícula de hiperparámetros que deseas optimizar
param_grid = {
    'max_depth': [None, 10, 20, 30],  # Profundidad máxima del árbol
    'min_samples_split': [2, 5, 10],  # Número mínimo de muestras requeridas para dividir un nodo interno
    'min_samples_leaf': [1, 2, 4]     # Número mínimo de muestras requeridas en un nodo hoja
}

# Crear un objeto GridSearchCV
grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')

# Realizar la búsqueda en cuadrícula en los datos de entrenamiento
grid_search.fit(X_train, y_train)

# Obtener los mejores hiperparámetros encontrados por la búsqueda en cuadrícula
best_params = grid_search.best_params_
print("Mejores hiperparámetros encontrados:", best_params)

# Entrenar el modelo con los mejores hiperparámetros
best_dt_model = DecisionTreeRegressor(**best_params)
best_dt_model.fit(X_train, y_train)

# Realizar predicciones con el modelo optimizado
dt_prediction = best_dt_model.predict(X_test)

mape=mean_absolute_percentage_error(y_test,dt_prediction)
mae=mean_absolute_error(y_test,dt_prediction)
print(mape)
print(mae)

